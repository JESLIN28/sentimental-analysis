{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d17e4e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SENTIMENTAL ANALYSIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e454b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install textblob\n",
    "!pip install wordcloud\n",
    "\n",
    "from warnings import filterwarnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from textblob import Word, TextBlob\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "filterwarnings(\"ignore\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.width\", 500)\n",
    "pd.set_option(\"display.float_format\", lambda x: '%.2f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddc4408",
   "metadata": {},
   "outputs": [],
   "source": [
    "df= pd.read_csv(r\"C:/Users/User/Jeslin_data/MO.csv\", encoding='latin-1')\n",
    "print('Read file 1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec10b974",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1de5bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8cbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f192c710",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50cab87",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c85f598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0828af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e1374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f23969b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed32bf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078bc983",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc1d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f197f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e9b130",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEXT PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e910e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(dataframe, dependent_var):\n",
    "  # Normalizing Case Folding - Uppercase to Lowercase\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x.lower() for x in str(x).split()))\n",
    "\n",
    "  # Removing Punctuation\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].str.replace('[^\\w\\s]','')\n",
    "\n",
    "  # Removing Numbers\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].str.replace('\\d','')\n",
    "\n",
    "  # StopWords\n",
    "  sw = stopwords.words('english')\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n",
    "\n",
    "  # Remove Rare Words\n",
    "  temp_df = pd.Series(' '.join(dataframe[dependent_var]).split()).value_counts()\n",
    "  drops = temp_df[temp_df <= 1]\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join(x for x in str(x).split() if x not in drops))\n",
    "\n",
    "  # Lemmatize\n",
    "  dataframe[dependent_var] = dataframe[dependent_var].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n",
    "\n",
    "  return dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a86613d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = text_preprocessing(df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a05952",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39afb35a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1049813",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200ca01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_visulaization(dataframe, dependent_var, barplot=True, wordcloud=True):\n",
    "  # Calculation of Term Frequencies\n",
    "  tf = dataframe[dependent_var].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n",
    "  tf.columns = [\"words\", \"tf\"]\n",
    "\n",
    "  if barplot:\n",
    "    # Bar Plot\n",
    "    tf[tf[\"tf\"]>1000].plot.barh(x=\"words\", y=\"tf\")\n",
    "    plt.title(\"Calculation of Term Frequencies : barplot\")\n",
    "    plt.show()\n",
    "    \n",
    "    if wordcloud:\n",
    "    # WordCloud\n",
    "     text = \" \".join(i for i in dataframe[dependent_var])\n",
    "    wordcloud = WordCloud(max_font_size=100, max_words=1000, background_color=\"white\").generate(text)\n",
    "    plt.figure(figsize=[10, 10])\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Calculation of Term Frequencies : wordcloud\")\n",
    "    plt.show()\n",
    "    wordcloud.to_file(\"wordcloud.png\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcd8f90",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "text_visulaization(df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad89e86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bfb8823",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_polarity_scores(dataframe, dependent_var):\n",
    "  sia = SentimentIntensityAnalyzer()\n",
    "  dataframe[\"polarity_score\"] = dataframe[dependent_var].apply(lambda x: sia.polarity_scores(x)[\"compound\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1225be",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_polarity_scores(df, \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bb85d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1bf412",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1316ba7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Lables\n",
    "def create_label(dataframe, dependent_var, independent_var):\n",
    "  sia = SentimentIntensityAnalyzer()\n",
    "  dataframe[independent_var] = dataframe[dependent_var].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\n",
    "  dataframe[independent_var] = LabelEncoder().fit_transform(dataframe[independent_var])\n",
    "\n",
    "  X = dataframe[dependent_var]\n",
    "  y = dataframe[independent_var]\n",
    "\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6062f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = create_label(df, \"text\", \"sentiment_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cd58d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split Dataset\n",
    "def split_dataset(dataframe, X, y):\n",
    "  train_x, test_x, train_y, test_y = train_test_split(X, y, random_state=1)\n",
    "  return train_x, test_x, train_y, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b42c629f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = split_dataset(df, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752f5112",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_count(train_x, test_x):\n",
    "  # Count Vectors\n",
    "  vectorizer = CountVectorizer()\n",
    "  x_train_count_vectorizer = vectorizer.fit_transform(train_x)\n",
    "  x_test_count_vectorizer = vectorizer.fit_transform(test_x)\n",
    "\n",
    "  return x_train_count_vectorizer, x_test_count_vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955c335b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065b5f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_TFIDF_word(train_x, test_x):\n",
    "  # TF-IDF word\n",
    "  tf_idf_word_vectorizer = TfidfVectorizer()\n",
    "  x_train_tf_idf_word = tf_idf_word_vectorizer.fit_transform(train_x)\n",
    "  x_test_tf_idf_word = tf_idf_word_vectorizer.fit_transform(test_x)\n",
    "\n",
    "  return x_train_tf_idf_word, x_test_tf_idf_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1047569d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f762ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_TFIDF_ngram(train_x, test_x):\n",
    "  # TF-IDF ngram\n",
    "  tf_idf_ngram_vectorizer = TfidfVectorizer(ngram_range=(2,3))\n",
    "  x_train_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(train_x)\n",
    "  x_test_tf_idf_ngram = tf_idf_ngram_vectorizer.fit_transform(test_x)\n",
    "\n",
    "  return x_train_tf_idf_ngram, x_test_tf_idf_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9c50475",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26107d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features_TFIDF_chars(train_x, test_x):\n",
    "  # TF-IDF Characters\n",
    "  tf_idf_chars_vectorizer = TfidfVectorizer(analyzer=\"char\", ngram_range=(2,3))\n",
    "  x_train_tf_idf_chars = tf_idf_chars_vectorizer.fit_transform(train_x)\n",
    "  x_test_tf_idf_chars = tf_idf_chars_vectorizer.fit_transform(test_x)\n",
    "\n",
    "  return x_train_tf_idf_chars, x_test_tf_idf_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c743115",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_tf_idf_chars, x_test_tf_idf_chars = create_features_TFIDF_chars(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f68ee4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Modeling - Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c5be67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "def crate_model_logistic(train_x, test_x):\n",
    "  # Count\n",
    "  x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "  loj_count = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "  loj_model_count = loj_count.fit(x_train_count_vectorizer, train_y)\n",
    "  accuracy_count = cross_val_score(loj_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "  # TF-IDF Word\n",
    "  x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)\n",
    "  loj_word = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "  loj_model_word = loj_word.fit(x_train_tf_idf_word, train_y)\n",
    "  accuracy_word = cross_val_score(loj_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "    \n",
    "  # TF-IDF ngram\n",
    "  x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)\n",
    "  loj_ngram = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "  loj_model_ngram = loj_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "  accuracy_ngram = cross_val_score(loj_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "  print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "\n",
    "  # TF-IDF chars\n",
    "\n",
    "  loj_chars = LogisticRegression(solver='lbfgs', max_iter=1000)\n",
    "  loj_model_chars = loj_chars.fit(x_train_tf_idf_chars, train_y)\n",
    "  accuracy_chars = cross_val_score(loj_model_chars, x_test_tf_idf_chars, test_y, cv=10).mean()\n",
    "  print(\"Accuracy TF-IDF Characters: %.3f\" % accuracy_chars)\n",
    "\n",
    "  return loj_model_count, loj_model_word, loj_model_ngram, loj_model_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af52eb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "loj_model_count, loj_model_word, loj_model_ngram, loj_model_chars = crate_model_logistic(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ec5b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "def crate_model_randomforest(train_x, test_x):\n",
    "  # Count\n",
    "  x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "  rf_count = RandomForestClassifier()\n",
    "  rf_model_count = rf_count.fit(x_train_count_vectorizer, train_y)\n",
    "  accuracy_count = cross_val_score(rf_model_count, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "\n",
    "  # TF-IDF Word\n",
    "  x_train_tf_idf_word, x_test_tf_idf_word = create_features_TFIDF_word(train_x, test_x)\n",
    "  rf_word = RandomForestClassifier()\n",
    "  rf_model_word = rf_word.fit(x_train_tf_idf_word, train_y)\n",
    "  accuracy_word = cross_val_score(rf_model_word, x_test_tf_idf_word, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - TF-IDF Word: %.3f\" % accuracy_word)\n",
    "\n",
    "  # TF-IDF ngram\n",
    "  x_train_tf_idf_ngram, x_test_tf_idf_ngram = create_features_TFIDF_ngram(train_x, test_x)\n",
    "  rf_ngram = RandomForestClassifier()\n",
    "  rf_model_ngram = rf_ngram.fit(x_train_tf_idf_ngram, train_y)\n",
    "  accuracy_ngram = cross_val_score(rf_model_ngram, x_test_tf_idf_ngram, test_y, cv=10).mean()\n",
    "  print(\"Accuracy TF-IDF ngram: %.3f\" % accuracy_ngram)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "  # TF-IDF chars\n",
    "\n",
    "  rf_chars = RandomForestClassifier()\n",
    "  rf_model_chars = rf_chars.fit(x_train_tf_idf_chars, train_y)\n",
    "  accuracy_chars = cross_val_score(rf_model_chars, x_test_tf_idf_chars, test_y, cv=10).mean()\n",
    "  print(\"Accuracy TF-IDF Characters: %.3f\" % accuracy_chars)\n",
    "\n",
    "  return rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd366c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_count, rf_model_word, rf_model_ngram, rf_model_chars = crate_model_randomforest(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb9908a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MODEL TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241d307b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_tuning_randomforest(train_x, test_x):\n",
    "  # Count\n",
    "  x_train_count_vectorizer, x_test_count_vectorizer = create_features_count(train_x, test_x)\n",
    "  rf_model_count = RandomForestClassifier(random_state=1)\n",
    "  rf_params = {\"max_depth\": [2,5,8, None],\n",
    "               \"max_features\": [2,5,8, \"auto\"],\n",
    "               \"n_estimators\": [100,500,1000],\n",
    "               \"min_samples_split\": [2,5,10]}\n",
    "  rf_best_grid = GridSearchCV(rf_model_count, rf_params, cv=10, n_jobs=-1, verbose=False).fit(x_train_count_vectorizer, train_y)\n",
    "  rf_model_count_final = rf_model_count.set_params(**rf_best_grid.best_params_, random_state=1).fit(x_train_count_vectorizer, train_y)\n",
    "  accuracy_count = cross_val_score(rf_model_count_final, x_test_count_vectorizer, test_y, cv=10).mean()\n",
    "  print(\"Accuracy - Count Vectors: %.3f\" % accuracy_count)\n",
    "    \n",
    "    return rf_model_count_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb56699",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model_count_final = model_tuning_randomforest(train_x, test_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb19e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cee6f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5a82f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_count(train_x, model, new_comment):\n",
    "  new_comment= pd.Series(new_comment)\n",
    "  new_comment = CountVectorizer().fit(train_x).transform(new_comment)\n",
    "  result = model.predict(new_comment)\n",
    "  if result==1:\n",
    "    print(\"Comment is Pozitive\")\n",
    "  else:\n",
    "    print(\"Comment is Negative\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f3c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "predict_count(train_x, model=loj_model_count, new_comment=\"this product is very good :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a2fd77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "predict_count(train_x, model=rf_model_count, new_comment=\"this product is very bad :)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f88672d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Review\n",
    "new_comment=pd.Series(df[\"text\"].sample(1).values)\n",
    "new_comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample Review - Random Forest\n",
    "predict_count(train_x, model=rf_model_count, new_comment=new_comment)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
